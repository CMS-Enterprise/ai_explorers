{"cells":[{"cell_type":"markdown","source":["## Install MLFlow\nInstall and then import the necessary packages."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8add6c87-3249-44d3-a41c-965bb73b8690"}}},{"cell_type":"code","source":["from tqdm import tqdm\nimport mlflow\nfrom mlflow import log_metric\n\nimport tempfile\nimport urllib3\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\nimport numpy as np \nimport pandas as pd\n\nimport os\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nimport torch\n\nfrom sklearn import metrics\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning import Trainer, seed_everything"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f947de74-6396-43ed-89a7-00bcff5d1908"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Model Code\n### Model setup\nThe MLP Model is instantiated with a number of variables.\n\nlearning_rate=.0001  - the learning rate is the manitude at which the gradient is applied to each parameter. 1e-3 is a standard default value. A LR Optimizer can change this. We implemented one that lowers the LR after a plateau in performance during training.\n\ndropout=.2 - the percentage of random parameters to ignore during training updates. This helps prevent overfitting, as well as improves generalization. As any parameter could be ignored during at specific training step, dropout forces the model to identify important patterns throughout the parameters, as opposed to routing all the info into certain paths.\n\nlayer_sizes=[128,128,128,64,32,16,8,4] - A list that the model uses to instantiate the dense layers. \n\ncriterion=None - This is the loss function - for us, generally BCELoss or BCELossWithLogits, as this is a binary classification problem. BCELWL expects no sigmoid applied to the last layer, so there is some code checking for that internally, scouting for the BCELoss class specifically. Both of those loss functions are the same underlying formula - Binary Cross Entropy - but BCELossWithLogits makes it a bit easier to upweight positive samples programatically. \n\ndevice=\"cpu\" - \"cpu\" or \"gpu\" to direct the model to what hardware to use. GPUs speed up training by orders of magnitude. \n\nweight_decay=0 - the coefficient for weight decay, which is implemented programatically in the Adam Optimizer\n\ninput_size=149 - the size of the feature vector input into the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"007e65a2-5c45-4349-a12c-c93f223f2022"}}},{"cell_type":"code","source":["class BasicMLP(pl.LightningModule):\n    def __init__(self, learning_rate=.0001, dropout=.2, layer_sizes=[128,128,128,64,32,16,8,4], criterion=None, device=\"cpu\",weight_decay=0, input_size = 149, pred_threshold = .5):\n        super(BasicMLP, self).__init__()\n        self.dvc = device if device == \"cpu\" else \"cuda\"\n        self.input_size = input_size\n        self.big_preds = []\n        self.hidden_act = nn.ReLU()\n        self.hidden_drop = nn.Dropout(p=dropout)    \n        self.fc_layers = self.init_fc_layers(layer_sizes)\n        self.out = nn.Linear(layer_sizes[-1], 1, self.dvc)        \n        self.out_activation = torch.nn.Sigmoid()\n        self.criterion = criterion\n        self.weight_decay = weight_decay\n        self.learning_rate = learning_rate\n        self.pred_threshold = pred_threshold\n  \n    def init_fc_layers(self, layer_sizes):   \n        fc_layers = [nn.Linear(self.input_size, layer_sizes[0], device=self.dvc)]\n        for i in range(len(layer_sizes)):\n          if i == 0:\n            continue\n          print(f\"Layer {i} created with dimensions {layer_sizes[i-1]},{layer_sizes[i]}\")\n          fc_layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i], device=self.dvc))\n        return nn.ModuleList(fc_layers)\n      \n    def needs_sigmoid(self):\n        # BCELoss need sigmoid, but BCELossWithLogits does not\n        return isinstance(self.criterion, nn.BCELoss)\n      \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        # TODO : change these to be parameterizable\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.1, patience=3, verbose=True)\n        return {\"optimizer\":optimizer, \n                \"lr_scheduler\":\n                    {\n                        \"scheduler\": lr_scheduler,\n                        \"monitor\": \"train_loss\",\n                        \"frequency\": 1\n                    }\n               }\n      \n    def forward(self, input):\n        e = input\n        for fc in self.fc_layers[:-1]:\n            e = self.hidden_drop(self.hidden_act(fc(e)))\n        e = self.hidden_act(self.fc_layers[-1](e))\n        if self.needs_sigmoid() or not self.training:\n          result = self.out_activation(self.out(e).squeeze(-1)) \n        else:\n          result = self.out(e).squeeze(-1)\n        return result\n      \n    def training_step(self, batch):\n        \"\"\"\n          The code for a training step. It's meant to be extremely similar to the validation step.\n          Note: this function needs to return either the raw loss value, or a dictionary with a \"loss\" entry.\n          This is an overwrite of a PTL function, and is required for PTL to work properly.\n        \"\"\"\n        ts, labels = batch\n        preds = self.forward(ts.float())\n        labels = labels.float()\n        metric_dict = self.calculate_metrics(preds, labels)\n        self.log_values(\"train\", metric_dict, labels, preds)\n        return metric_dict\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n          The code for a validation step. It's meant to be extremely similar to the training step.\n          Note: this function needs to return either the raw loss value, or a dictionary with a \"loss\" entry.\n          This is an overwrite of a PTL function, and is required for PTL to work properly.\n        \"\"\"\n        ts, labels = batch\n        preds = self.forward(ts.float())\n        labels = labels.float()\n        metric_dict = self.calculate_metrics(preds, labels)\n        self.log_values(\"val\", metric_dict, labels, preds)\n        return metric_dict\n      \n    def calculate_metrics(self, preds, labels):\n        \"\"\"\n          A function to process and accumulate metrics from predictions and labels. \n        \"\"\"\n        metric_dict = {}\n        # Take the loss - if sigmoid needed to be applied, it will have already happened in the forward pass\n        metric_dict[\"loss\"] = self.criterion(preds, labels)\n        # Apply sigmoid if needed\n        if self.needs_sigmoid():\n          local_preds = [pred for pred in torch.sigmoid(preds).cpu().tolist()]\n        else:\n          local_preds = [pred for pred in preds.cpu().tolist()]\n        \n        \n        # Set threshold so that the top self.pred_threshold proportion of predictions are predicted as true\n        lp = local_preds\n        lp.sort(reverse = True)\n        threshold = lp[ round(len(lp)*self.pred_threshold)]\n        \n        non_tensor_preds = []\n        # Round to turn predictions into labels\n        for pred in local_preds:\n          if pred < threshold:\n            pred = 0.0\n          else:\n            pred = 1.0\n          non_tensor_preds.append(pred)\n#         non_tensor_preds = [min(round(max(pred,0)), 1) for pred in local_preds]\n        # Put tensor on cpu and cast to numpy, as tensors are not processable by sklearn \n        non_tensor_labels = labels.cpu().tolist()\n        self.big_preds.extend(non_tensor_preds)\n        metric_dict[\"f1\"] = metrics.f1_score(non_tensor_labels, non_tensor_preds, zero_division=0, pos_label=1.0, average=\"binary\")\n        metric_dict[\"precision\"] = metrics.precision_score(non_tensor_labels, non_tensor_preds, zero_division=0, pos_label=1.0, average=\"binary\")\n        metric_dict[\"recall\"] = metrics.recall_score(non_tensor_labels, non_tensor_preds, zero_division=0, pos_label=1.0, average=\"binary\")\n        metric_dict['pr_auc'] = metrics.average_precision_score(non_tensor_labels, local_preds, pos_label=1.0, average=\"micro\")\n        return metric_dict\n  \n    def log_values(self, train_or_val, metric_dict, labels, preds):\n        # log all metrics in mlflow and ptl logger \n        for key, value in metric_dict.items(): \n            if key == \"loss\":\n                self.log(f\"{train_or_val}_{key}\",float(value),prog_bar=True)\n            else:\n                self.log(f\"{train_or_val}_{key}\",float(value))\n            log_metric(f\"{train_or_val}_{key}\",float(value))\n        \n    def gather_mean(self, dict_key, val_dicts):\n        metric_list = np.stack([batch[dict_key] for batch in val_dicts if dict_key in batch])\n        mean = metric_list.mean()\n        log_metric(f\"val_epoch_end_{dict_key}\", mean)\n    \n    def validation_epoch_end(self, val_outputs):\n        \"\"\"\n          Val outputs is a list of all validation_step outputs. Each Element will be the output of one validation batch\n        \"\"\"\n        self.gather_mean(\"f1\", val_outputs)\n        self.gather_mean(\"precision\", val_outputs)\n        self.gather_mean(\"recall\", val_outputs)\n        self.gather_mean(\"pr_auc\", val_outputs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d19940b2-8108-44be-b450-61871f49b3b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import shutil\nclass MLFlowTrainer(Trainer):\n    \"\"\"\n      MLFlowTrainer - a class to imitate a PTL trainer, but also save a model to a persistent location when it logs a checkpoint artifact\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def save_checkpoint(self, filepath, weights_only):\n        # we still want all the normal functionality of the ptl save_checkpoint function\n        super().save_checkpoint(filepath, weights_only)\n        run_id = mlflow.active_run().info.run_id\n        basename = os.path.basename(filepath)\n        # derive the path to save the checkpoint\n        run_dir = f\"/dbfs/mnt/eldb_mnt/MMA394/model_storage/{run_id}\" \n        try:\n          os.makedirs(run_dir, exist_ok=True)\n          if os.path.isfile(filepath):\n              # log the model checkpoint as an MLflow object, then copy it over to the new location\n              mlflow.log_artifact(local_path=filepath) \n              shutil.copyfile(filepath, f\"{run_dir}/{basename}\")\n          else:\n              print(f\"Could not find {filepath} when saving checkpoint\")\n        except Exception as e:\n          # we were troubleshooting our training run failures and were considering\n          # that saving directly to the filesytem could be messing with things. It\n          # did not seem to be the case, tho we left the try/catch in as we've\n          # encountered other errors when saving the model before.\n          print(f\"couldn't save model to: {filepath}\")\n          print(f\"Error: {e}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44325e43-95c5-4b9e-821e-ed0e85f821f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# !ls /dbfs/mnt/eldb_mnt/MMA394/model_storage/e370befd10604f92a4ab91a75b346cb9"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49e789ff-1c15-4d1b-a5ac-a57a42ae5606"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&#39;epoch=0-step=892.ckpt&#39;    &#39;epoch=3-step=3568.ckpt&#39;  &#39;epoch=7-step=7136.ckpt&#39;\r\n&#39;epoch=1-step=1784.ckpt&#39;   &#39;epoch=4-step=4460.ckpt&#39;  &#39;epoch=8-step=8028.ckpt&#39;\r\n&#39;epoch=10-step=9812.ckpt&#39;  &#39;epoch=5-step=5352.ckpt&#39;  &#39;epoch=9-step=8920.ckpt&#39;\r\n&#39;epoch=2-step=2676.ckpt&#39;   &#39;epoch=6-step=6244.ckpt&#39;\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&#39;epoch=0-step=892.ckpt&#39;    &#39;epoch=3-step=3568.ckpt&#39;  &#39;epoch=7-step=7136.ckpt&#39;\r\n&#39;epoch=1-step=1784.ckpt&#39;   &#39;epoch=4-step=4460.ckpt&#39;  &#39;epoch=8-step=8028.ckpt&#39;\r\n&#39;epoch=10-step=9812.ckpt&#39;  &#39;epoch=5-step=5352.ckpt&#39;  &#39;epoch=9-step=8920.ckpt&#39;\r\n&#39;epoch=2-step=2676.ckpt&#39;   &#39;epoch=6-step=6244.ckpt&#39;\r\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ade56e12-4d75-494f-9d42-209e60d3c88b"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"model_setup","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":393219}},"nbformat":4,"nbformat_minor":0}
