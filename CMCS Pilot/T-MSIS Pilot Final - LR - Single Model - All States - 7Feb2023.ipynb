{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad6ff7d-b6a4-43af-9b8c-18af6d1f2190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pyspark.sql.functions import col\n",
    "\n",
    "def get_date_int(in_date):\n",
    "    return int(str(in_date).split('-')[2])\n",
    "  \n",
    "\n",
    "def get_date_text(in_date):\n",
    "    return str(in_date)\n",
    "\n",
    "def run_liner_regression(train_pdf,train_pdf_work,test_pdf,process_type,state_numeric,state_alpha,file,run_nbr):\n",
    "    train_pdf['date_int'] = train_pdf.daily_rep_pd_date.apply(get_date_int)\n",
    "    test_pdf['date_int'] = test_pdf.daily_rep_pd_date.apply(get_date_int)\n",
    "    train_pdf_work['date_int'] = train_pdf_work.daily_rep_pd_date.apply(get_date_int)\n",
    "\n",
    "    x_train = train_pdf_work.copy()\n",
    "    x_test = test_pdf.copy()\n",
    "    y_train = x_train[['errs']]\n",
    "    y_test = x_test[['errs']]\n",
    "\n",
    "    #Drop errors from the x dataset\n",
    "    x_train=x_train.drop('errs',axis=1)\n",
    "    x_test=x_test.drop('errs',axis=1)\n",
    "    x_train=x_train.drop('daily_rep_pd_date',axis=1)\n",
    "    x_test=x_test.drop('daily_rep_pd_date',axis=1)\n",
    "\n",
    "    #Fit Linear model\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(x_train,y_train)\n",
    "\n",
    "    #Model Scores\n",
    "    model_score_train = regression_model.score(x_train,y_train)\n",
    "    #print(\"Model Score on Train set : \" + str(model_score_train) )\n",
    "\n",
    "    model_score_test = regression_model.score(x_test,y_test)\n",
    "    #print(\"Model Score on Test set : \" + str(model_score_test) )\n",
    "\n",
    "    y_predict = pd.DataFrame(regression_model.predict(x_test))\n",
    "    y_predict.rename(columns = {0:'errs_predict'}, inplace = True)\n",
    "\n",
    "    lr_df = pd.concat([test_pdf,y_predict], axis=1, join='inner')\n",
    "    \n",
    "    lr_df['file_type'] = file\n",
    "    lr_df['state_code_numeric'] = state_numeric\n",
    "    lr_df['state_code_alpha'] = state_alpha\n",
    "    lr_df['date_text'] = lr_df.daily_rep_pd_date.apply(get_date_text)\n",
    "\n",
    "    lr_df['model_score_train'] = str(model_score_train)[:4]\n",
    "    lr_df['model_score_validate'] = ' ' \n",
    "    lr_df['model_score_test'] = str(model_score_test)[:4]\n",
    "\n",
    "    lr_df['model_parent_process'] = 'Python ML'\n",
    "    lr_df['model_name'] = 'Linear regression'\n",
    "    lr_df['score_type'] = 'R2'\n",
    "    lr_df['errs_predict'] = lr_df['errs_predict'].astype('int64')\n",
    "\n",
    "    lr_df[\"best_model_url\"] = ''\n",
    "    lr_df[\"experimant_info\"] = ''\n",
    "    lr_df[\"model_run_num\"] = run_nbr\n",
    "    lr_df['model_run_type'] = 'C - LR - Single Model'\n",
    "    lr_df['comments'] = process_type\n",
    "    lr_df['source_data_table_name'] = ' '\n",
    "    \n",
    "    lr_df['errors_actual_previous_mo'] = 0\n",
    "    lr_df['mo_to_mo_difference'] = 0\n",
    "    #lr_df['errors_predicted_current_mo'] = 0\n",
    "    lr_df['errors_actual_previous_mo'] = 0\n",
    "    lr_df['errors_actual_previous_mo'] = lr_df['errors_actual_previous_mo'].astype('int64')\n",
    "    lr_df['actual_to_predicted_difference'] = 0\n",
    "    lr_df['source_notebook_url'] = ' '\n",
    "    lr_df['actual_to_predicted_pct'] = 0\n",
    "    lr_df['experiment_info'] = ' '\n",
    "    lr_df['mo_to_mo_pct'] = 0\n",
    "    #lr_df['errors_actual_current_mo'] = 0\n",
    "    lr_df.rename(columns={'errs': 'errors_actual_current_mo', 'errs_predict': 'errors_predicted_current_mo'}, inplace=True)\n",
    "    lr_df['model_score_forecast'] = 0\n",
    "    lr_df['absolute_difference'] = 0\n",
    "    \n",
    "    return_df=lr_df[['model_run_type','model_run_num','state_code_numeric','state_code_alpha','daily_rep_pd_date','date_text','date_int','file_type','errors_actual_previous_mo','errors_actual_current_mo','mo_to_mo_difference',\t'mo_to_mo_pct','errors_predicted_current_mo','actual_to_predicted_difference','absolute_difference','actual_to_predicted_pct','model_parent_process','model_name','score_type','model_score_forecast','model_score_train','model_score_validate','model_score_test','source_notebook_url','best_model_url','experiment_info','source_data_table_name','comments']]\n",
    "    \n",
    "    spark_return_df = spark.createDataFrame(return_df) \n",
    "    spark_return_df.write.saveAsTable(\"datalab_scratch.lr_test_forecast_1_all_kv\",mode=\"append\")\n",
    "    \n",
    "    train_pdf['model_run_type'] = 'C - LR - Single Model'\n",
    "    train_pdf['file_type'] = file\n",
    "    train_pdf['state_code_numeric'] = state_numeric\n",
    "    train_pdf['state_code_alpha'] = state_alpha\n",
    "    train_pdf['comments'] = process_type\n",
    "    train_pdf['model_run_num'] = run_nbr\n",
    "    train_pdf['errs'] = train_pdf['errs'].astype('int64')\n",
    "    \n",
    "    return_train_pdf = train_pdf[['model_run_type','model_run_num','state_code_numeric','state_code_alpha','daily_rep_pd_date','date_int','file_type','comments','errs']]\n",
    "    spark_train_pdf = spark.createDataFrame(return_train_pdf) \n",
    "    spark_train_pdf.write.saveAsTable(\"datalab_scratch.lr_train_forecast_1_all_kv\",mode=\"append\")\n",
    "    \n",
    "    \n",
    "def get_train_and_test_df(table_name):\n",
    "    query_1 = \"select * from \" + table_name + \" where daily_rep_pd_date >= '2022-01-01' AND daily_rep_pd_date <= '2022-01-18'\"\n",
    "    query_2 = \"select * from \" + table_name + \" where daily_rep_pd_date >= '2022-01-19'\"\n",
    "    \n",
    "    train_df = spark.sql(query_1)\n",
    "    train_pdf = train_df.select(\"*\").toPandas()\n",
    "    \n",
    "    test_df = spark.sql(query_2)\n",
    "    test_pdf = test_df.select(\"*\").toPandas()\n",
    "    \n",
    "    return train_pdf, test_pdf\n",
    "\n",
    "def treat_outliers(df, col):\n",
    "    \"\"\"\n",
    "    treats outliers in a variable\n",
    "    col: str, name of the numerical variable\n",
    "    df: dataframe\n",
    "    col: name of the column\n",
    "    \"\"\"\n",
    "    Q1 = df[col].quantile(0.25)  # 25th quantile\n",
    "    Q3 = df[col].quantile(0.75)  # 75th quantile\n",
    "    IQR = Q3 - Q1                # Inter Quantile Range (75th perentile - 25th percentile)\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "    # all the values smaller than lower_whisker will be assigned the value of lower_whisker\n",
    "    # all the values greater than upper_whisker will be assigned the value of upper_whisker\n",
    "    # the assignment will be done by using the clip function of NumPy\n",
    "    df[col] = np.clip(df[col], lower_whisker, upper_whisker)\n",
    "\n",
    "    return df\n",
    "\n",
    "#drop result table\n",
    "\n",
    "try:\n",
    "  spark.sql(\"drop table datalab_scratch.lr_train_forecast_1_all_kv \")\n",
    "except Exception as e:\n",
    "\n",
    "  print(f\"Table does not esist\")\n",
    "  print(e) \n",
    "\n",
    "try:\n",
    "  spark.sql(\"drop table datalab_scratch.lr_test_forecast_1_all_kv \")\n",
    "except Exception as e:\n",
    "\n",
    "  print(f\"Table does not esist\")\n",
    "  print(e) \n",
    "\n",
    "state_df_spark = spark.sql(\"select * from datalab_scratch.state_codes where state_code_numeric in ('01','02','04','05','06','08','09','10','11','12','13','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','44','45','46','47','48','49','50','51','53','54','55','56','60','64','66','68','69','70','72','74','78','93','94','96','97') order by State_Code_Numeric\")\n",
    "\n",
    "state_df = state_df_spark.toPandas()\n",
    "\n",
    "file_list = ['CIP','CLT','COT','CRX','ELG','MCR','PRV','TPL']\n",
    "\n",
    "process_type_list = ['Features/Processing: submission_methods','Features/Processing: submission_methods_with_outlier_treatment','Features/Processing: error_category','Features/Processing: error_category_with_outlier_treatment']\n",
    "run_nbr = 0\n",
    "\n",
    "      \n",
    "for process_type in process_type_list:\n",
    "    print(process_type)\n",
    "    run_nbr = run_nbr + 1\n",
    "    for index, row in state_df.iterrows():\n",
    "        state_numeric = row.State_Code_Numeric\n",
    "        state_alpha = row.State_Code_Alpha\n",
    "        print(state_numeric)\n",
    "        print(state_alpha)\n",
    "        for file in file_list:\n",
    "            print(file)\n",
    "            if 'submission_methods' in process_type:\n",
    "               table_name = \"datalab_scratch.daily_\" + state_numeric + \"_\" + state_alpha + \"_\" + file + \"_ready_ml_input_df_ss\"\n",
    "            else:\n",
    "               table_name = \"datalab_scratch.daily_\" + state_numeric + \"_\" + state_alpha + \"_\" + file + \"_ready_ml_input_df\"\n",
    "            \n",
    "            train_pdf, test_pdf = get_train_and_test_df(table_name)\n",
    "            train_pdf_work = train_pdf.copy()\n",
    "            #train_pdf['errs_orig'] = train_pdf['errs']\n",
    "            \n",
    "            if 'outlier' in process_type:\n",
    "               train_pdf_work = treat_outliers(train_pdf_work,'errs')\n",
    "              \n",
    "            if train_pdf.empty or test_pdf.empty:\n",
    "               print(\"No Data for the state \" + state_alpha + \" and file type \" + file )\n",
    "            else:\n",
    "               run_liner_regression(train_pdf,train_pdf_work,test_pdf,process_type,state_numeric,state_alpha,file,run_nbr)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e3c35da-7fe8-4d39-b9ab-619fea9b0a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table datalab_scratch.lr_test_forecast_2_all_kv using delta\n",
    "as\n",
    "select model_run_type\n",
    "     , model_run_num\n",
    "     , state_code_numeric\n",
    "     , state_code_alpha\n",
    "     , daily_rep_pd_date\n",
    "     , date_text\n",
    "     , date_int\n",
    "     , file_type\n",
    "     , sum(errors_actual_previous_mo) as errors_actual_previous_mo\n",
    "     , sum(errors_actual_current_mo) as errors_actual_current_mo\n",
    "     , sum(mo_to_mo_difference) as mo_to_mo_difference\n",
    "     , sum(mo_to_mo_pct) as mo_to_mo_pct\n",
    "     , sum(errors_predicted_current_mo) as errors_predicted_current_mo\n",
    "     , sum(actual_to_predicted_difference) as actual_to_predicted_difference\n",
    "     , sum(absolute_difference) as absolute_difference\n",
    "     , sum(actual_to_predicted_pct) as actual_to_predicted_pct\n",
    "     , model_parent_process\n",
    "     , model_name\n",
    "     , score_type\n",
    "     , model_score_forecast\n",
    "     , model_score_train\n",
    "     , model_score_validate\n",
    "     , model_score_test\n",
    "     , source_notebook_url\n",
    "     , best_model_url\n",
    "     , experiment_info\n",
    "     , source_data_table_name\n",
    "     , comments\n",
    "  from datalab_scratch.lr_test_forecast_1_all_kv\n",
    "-- where model_run_num = 3\n",
    "--   and state_code_numeric = '44'\n",
    "--   and file_type = 'COT'\n",
    " --  and date_int = 19\n",
    " group by model_run_type\n",
    "        , model_run_num\n",
    "        , state_code_numeric\n",
    "        , state_code_alpha\n",
    "        , daily_rep_pd_date\n",
    "        , date_text\n",
    "        , date_int\n",
    "        , file_type\n",
    "        , model_parent_process\n",
    "        , model_name\n",
    "        , score_type\n",
    "        , model_score_forecast\n",
    "        , model_score_train\n",
    "        , model_score_validate\n",
    "        , model_score_test\n",
    "        , source_notebook_url\n",
    "        , best_model_url\n",
    "        , experiment_info\n",
    "        , source_data_table_name\n",
    "        , comments\n",
    " order by model_run_num\n",
    "        , state_code_numeric\n",
    "        , state_code_alpha\n",
    "        , file_type\n",
    "        , date_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025d86ed-99f9-4997-9c58-1cf79689aa3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table datalab_scratch.lr_train_forecast_2_all_kv using delta\n",
    "as\n",
    "select model_run_type\n",
    "     , model_run_num\n",
    "     , state_code_numeric\n",
    "     , state_code_alpha\n",
    "     , daily_rep_pd_date\n",
    "     , date_int\n",
    "     , file_type\n",
    "     , comments\n",
    "     , sum(errs) as errs\n",
    "  from datalab_scratch.lr_train_forecast_1_all_kv\n",
    "-- where model_run_num = 3\n",
    "--   and state_code_numeric = '44'\n",
    "--   and file_type = 'COT'\n",
    "--   and date_int = 18\n",
    " group by model_run_type\n",
    "        , model_run_num\n",
    "        , state_code_numeric\n",
    "        , state_code_alpha\n",
    "        , daily_rep_pd_date\n",
    "        , date_int\n",
    "        , file_type\n",
    "        , comments\n",
    "  order by model_run_num\n",
    "        , state_code_numeric\n",
    "        , state_code_alpha\n",
    "        , file_type\n",
    "        , date_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c23ad0-46cb-4cd5-b6b8-6d31cc195a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "final_df = pd.DataFrame(columns=['model_run_type','model_run_num','state_code_numeric','state_code_alpha','daily_rep_pd_date','date_text','date_int','file_type','errors_actual_previous_mo','errors_actual_current_mo','mo_to_mo_difference',\t'mo_to_mo_pct','errors_predicted_current_mo','actual_to_predicted_difference','absolute_difference','actual_to_predicted_pct','model_parent_process','model_name','score_type','model_score_forecast','model_score_train','model_score_validate','model_score_test','source_notebook_url','best_model_url','experiment_info','source_data_table_name','comments'])\n",
    "\n",
    "\n",
    "try:\n",
    "  spark.sql(\"drop table datalab_scratch.lr_final_forecast_all_kv\")\n",
    "except Exception as e:\n",
    "#except:\n",
    "  print(f\"Table does not esist\")\n",
    "  print(e) \n",
    "\n",
    "test_df_spark = spark.sql(\"select * from datalab_scratch.lr_test_forecast_2_all_kv \")\n",
    "test_df = test_df_spark.toPandas()\n",
    "\n",
    "train_df_spark = spark.sql(\"select * from datalab_scratch.lr_train_forecast_2_all_kv \")\n",
    "train_df = train_df_spark.toPandas()\n",
    "\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    previous_month = row.date_int - 1\n",
    "    #print(previous_month)\n",
    "    if row.date_int == 19:\n",
    "       get_previous_month_errors = train_df.loc[(train_df['state_code_numeric'] == row.state_code_numeric) & (train_df['model_run_num'] == row.model_run_num) & (train_df['file_type'] == row.file_type) & (train_df['date_int'] == 18) ]['errs'].values[0]\n",
    "    else:\n",
    "       get_previous_month_errors = test_df.loc[(test_df['state_code_numeric'] == row.state_code_numeric) & (test_df['model_run_num'] == row.model_run_num) & (test_df['file_type'] == row.file_type) & (test_df['date_int'] == previous_month) ]['errors_actual_current_mo'].values[0]\n",
    "    #print(get_previous_month_errors)\n",
    "    \n",
    "    month_to_month_difference = row.errors_actual_current_mo - get_previous_month_errors\n",
    "    month_to_month_pct = float(month_to_month_difference/get_previous_month_errors)\n",
    "    \n",
    "    actual_to_predicted_difference_errors = row.errors_predicted_current_mo - row.errors_actual_current_mo\n",
    "    absolute_diff = abs(actual_to_predicted_difference_errors)\n",
    "    actual_to_predic_pct = float(actual_to_predicted_difference_errors/row.errors_actual_current_mo)\n",
    "    \n",
    "    \n",
    "    final_df = final_df.append({'model_run_type':row.model_run_type,'model_run_num':row.model_run_num,'state_code_numeric':row.state_code_numeric,'state_code_alpha':row.state_code_alpha,'daily_rep_pd_date':row.daily_rep_pd_date,'date_text':row.date_text,'date_int':row.date_int,'file_type':row.file_type,'errors_actual_previous_mo':get_previous_month_errors,'errors_actual_current_mo':row.errors_actual_current_mo,'mo_to_mo_difference':month_to_month_difference,\t'mo_to_mo_pct':month_to_month_pct,'errors_predicted_current_mo':row.errors_predicted_current_mo,'actual_to_predicted_difference':actual_to_predicted_difference_errors,'absolute_difference':absolute_diff,'actual_to_predicted_pct':actual_to_predic_pct,'model_parent_process':row.model_parent_process,'model_name':row.model_name,'score_type':row.score_type,'model_score_forecast':row.model_score_forecast,'model_score_train':row.model_score_train,'model_score_validate':row.model_score_validate,'model_score_test':row.model_score_test,'source_notebook_url':row.source_notebook_url,'best_model_url':row.best_model_url,'experiment_info':row.experiment_info,'source_data_table_name':row.source_data_table_name,'comments':row.comments},ignore_index=True)\n",
    "\n",
    "print(final_df)\n",
    "spark_final_df = spark.createDataFrame(final_df) \n",
    "spark_final_df.write.saveAsTable(\"datalab_scratch.lr_final_forecast_all_kv\",mode=\"overwrite\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ba0614-8340-47ac-a913-2add2cd0899f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from datalab_scratch.lr_final_forecast_all_kv\n",
    " where state_code_numeric = '29' \n",
    " order by file_type,date_int, model_run_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6974df26-374f-4f0f-94b1-079f64663848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update datalab_scratch.lr_final_forecast_all_kv\n",
    "   set source_data_table_name = 'datalab_scratch.lr_final_forecast_all_kv'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e60ff6-aaa7-4f84-a564-4aad9f1e6b1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update datalab_scratch.lr_final_forecast_all_kv\n",
    "   set source_notebook_url = 'https://databricks-val-data.macbisdw.cmscloud.local/#notebook/3091693/command/3104235'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0753386d-7a1c-4cfe-be8a-dbafab1b16da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from datalab_scratch.lr_final_forecast_all_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e96a81-8d80-4408-a1b4-af2f603e06c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select state_code_alpha, state_code_numeric, file_type, date_text, model_run_num, errors_actual_current_mo, actual_to_predicted_difference, actual_to_predicted_pct, comments\n",
    "from datalab_scratch.lr_final_forecast_all_kv\n",
    "where date_text = \"2022-01-19\"\n",
    "order by state_code_alpha, state_code_numeric, file_type, date_text, model_run_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec205593-d880-467a-8bf5-1d46985304bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from datalab_scratch.lr_final_forecast_all_kv\n",
    " where model_run_num = 1\n",
    "   and state_code_numeric = 44\n",
    " order by file_type,date_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c8e19f-e124-4279-9716-54811607962e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select avg(float(model_score_test)) from datalab_scratch.lr_final_forecast_all_kv\n",
    " where model_run_num = 1 and model_score_test<>'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44946bee-4cf6-4d82-b6cc-28da36860a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from datalab_scratch.lr_final_forecast_all_kv\n",
    "where model_run_num=1\n",
    "order by model_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d583b1-ecfa-4d56-9158-c740b7e02c3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql --unique combos\n",
    "select count(*) from\n",
    "(select distinct state_code_numeric, file_type from datalab_scratch.lr_final_forecast_all_kv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df265766-2f51-49cd-9383-214fa16229e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql -- All variations of feature generation had a significant number of models that scored worse than a constant function for a simple linear regression\n",
    "select model_run_num, count(*) from (\n",
    "select state_code_numeric, file_type, model_run_num, avg(model_score_test)\n",
    "from datalab_scratch.lr_final_forecast_all_kv\n",
    "group by state_code_numeric, file_type, model_run_num\n",
    "having avg(model_score_test) < 0)\n",
    "group by model_run_num\n",
    "order by 2 desc"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "T-MSIS Pilot Final - LR - Single Model - All States - 7Feb2023",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
